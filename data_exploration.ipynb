{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import check_accuracy_classification\n",
    "import transformers\n",
    "from torch.optim import Adam\n",
    "from models import BertProbeClassifer\n",
    "from utils import text_to_dataloader, tokenize_word\n",
    "from bert_embedding import BertEmbeddingExtractorRandom, BertEmbeddingExtractorVanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"data\",\"en_partut-ud-train.conllu\")\n",
    "dev_path = os.path.join(\"data\",\"en_partut-ud-dev.conllu\")\n",
    "test_path = os.path.join(\"data\",\"en_partut-ud-test.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER_CONST = \"# sent_id = \"\n",
    "TEXT_CONST = \"# text = \"\n",
    "STOP_CONST = \"\\n\"\n",
    "WORD_OFFSET = 1\n",
    "LABEL_OFFSET = 3\n",
    "NUM_OFFSET = 0\n",
    "\n",
    "\n",
    "def txt_to_dataframe(data_path):\n",
    "    '''\n",
    "    read UD text file and convert to df format\n",
    "    '''\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        df = pd.DataFrame(\n",
    "            columns={\n",
    "                \"text\",\n",
    "                \"word\",\n",
    "                \"label\"\n",
    "            }\n",
    "        )\n",
    "        for line in fp.readlines():\n",
    "            if TEXT_CONST in line:\n",
    "                words_list = []\n",
    "                labels_list = []\n",
    "                num_list = []\n",
    "                text = line.split(TEXT_CONST)[1]\n",
    "                # this is a new text, need to parse all the words in it\n",
    "            elif line is not STOP_CONST and HEADER_CONST not in line:\n",
    "                temp_list = line.split(\"\\t\")\n",
    "                num_list.append(temp_list[NUM_OFFSET])\n",
    "                words_list.append(temp_list[WORD_OFFSET])\n",
    "                labels_list.append(temp_list[LABEL_OFFSET])\n",
    "            if line == STOP_CONST:\n",
    "                # this is the end of the text, adding to df\n",
    "                cur_df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"text\": len(words_list) * [text],\n",
    "                        \"word\": words_list,\n",
    "                        \"word_offset\": num_list,\n",
    "                        \"label\": labels_list\n",
    "                    }\n",
    "                )\n",
    "                df = pd.concat([df,cur_df])\n",
    "        return df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = txt_to_dataframe(train_path)\n",
    "df_dev = txt_to_dataframe(dev_path)\n",
    "df_test = txt_to_dataframe(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = [\n",
    "    \"ADJ\",\n",
    "    \"ADP\",\n",
    "    \"ADV\",\n",
    "    \"AUX\",\n",
    "    \"CCONJ\",\n",
    "    \"DET\",\n",
    "    \"INTJ\",\n",
    "    \"NOUN\",\n",
    "    \"NUM\",\n",
    "    \"PART\",\n",
    "    \"PRON\",\n",
    "    \"PROPN\",\n",
    "    \"PUNCT\",\n",
    "    \"SCONJ\",\n",
    "    \"SYM\",\n",
    "    \"VERB\",\n",
    "    \"X\",\n",
    "    \"_\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes, Mrs Schroedter, I shall be pleased to loo...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>etc.</td>\n",
       "      <td>For this reason, one of the most important and...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>right</td>\n",
       "      <td>We know that, right?\\n</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>yes</td>\n",
       "      <td>In developing countries that have not yet reac...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes, being better connected with each other, t...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>etc.</td>\n",
       "      <td>in an answer to the sharers' petition in 1635 ...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word                                               text label word_offset\n",
       "0     Yes  Yes, Mrs Schroedter, I shall be pleased to loo...  INTJ           1\n",
       "62   etc.  For this reason, one of the most important and...  INTJ          63\n",
       "4   right                             We know that, right?\\n  INTJ           5\n",
       "17    yes  In developing countries that have not yet reac...  INTJ          18\n",
       "0     Yes  Yes, being better connected with each other, t...  INTJ           1\n",
       "48   etc.  in an answer to the sharers' petition in 1635 ...  INTJ          49"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train[\"label\"] == \"INTJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Type   |   Count |\n",
      "|:-------|--------:|\n",
      "| NOUN   |    9249 |\n",
      "| ADP    |    5220 |\n",
      "| PUNCT  |    5105 |\n",
      "| DET    |    4616 |\n",
      "| VERB   |    4126 |\n",
      "| ADJ    |    3410 |\n",
      "| AUX    |    2076 |\n",
      "| PROPN  |    2033 |\n",
      "| PRON   |    1734 |\n",
      "| ADV    |    1707 |\n",
      "| CCONJ  |    1472 |\n",
      "| PART   |    1168 |\n",
      "| NUM    |     787 |\n",
      "| SCONJ  |     627 |\n",
      "| X      |     140 |\n",
      "| SYM    |      42 |\n",
      "| _      |      27 |\n",
      "| INTJ   |       6 |\n"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/label_dist_train.tex'\n",
    "SORT_COL = \"Count\"\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_train[\"label\"].value_counts().rename_axis(\"Type\").to_frame(\"Count\").reset_index()\n",
    "    #display_df.index = TYPES\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True, ascending=False)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    tf.write(latex_data)\n",
    "    print(display_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Type   |   Count |\n",
      "|:-------|--------:|\n",
      "| NOUN   |     568 |\n",
      "| PUNCT  |     353 |\n",
      "| ADP    |     297 |\n",
      "| VERB   |     276 |\n",
      "| DET    |     266 |\n",
      "| ADJ    |     210 |\n",
      "| PRON   |     153 |\n",
      "| AUX    |     124 |\n",
      "| ADV    |     108 |\n",
      "| PROPN  |     107 |\n",
      "| CCONJ  |      88 |\n",
      "| NUM    |      60 |\n",
      "| PART   |      56 |\n",
      "| SCONJ  |      41 |\n",
      "| X      |      13 |\n",
      "| SYM    |       2 |\n",
      "| _      |       1 |\n"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/label_dist_dev.tex'\n",
    "\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_dev[\"label\"].value_counts().rename_axis(\"Type\").to_frame(\"Count\").reset_index()\n",
    "    #display_df.index = TYPES\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True, ascending=False)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    tf.write(latex_data)\n",
    "    print(display_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Type   |   Count |\n",
      "|:-------|--------:|\n",
      "| NOUN   |     753 |\n",
      "| ADP    |     488 |\n",
      "| DET    |     439 |\n",
      "| PUNCT  |     339 |\n",
      "| VERB   |     326 |\n",
      "| AUX    |     234 |\n",
      "| ADJ    |     224 |\n",
      "| ADV    |     131 |\n",
      "| PRON   |     106 |\n",
      "| CCONJ  |      96 |\n",
      "| PROPN  |      90 |\n",
      "| PART   |      66 |\n",
      "| NUM    |      61 |\n",
      "| SCONJ  |      51 |\n",
      "| _      |       4 |\n",
      "| X      |       2 |\n",
      "| INTJ   |       2 |\n"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/label_dist_test.tex'\n",
    "\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_test[\"label\"].value_counts().rename_axis(\"Type\").to_frame(\"Count\").reset_index()\n",
    "    #display_df.index = TYPES\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True, ascending=False)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    tf.write(latex_data)\n",
    "    print(display_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences:\n",
      "Train: 1780\n",
      "Dev:   156\n",
      "Test:  153\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Sentences:\")\n",
    "print(f\"Train: {len(df_train.drop_duplicates(subset='text', keep='first'))}\")\n",
    "print(f\"Dev:   {len(df_dev.drop_duplicates(subset='text', keep='first'))}\")\n",
    "print(f\"Test:  {len(df_test.drop_duplicates(subset='text', keep='first'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words:\n",
      "Train: 43545\n",
      "Dev:   2723\n",
      "Test:  3412\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Words:\")\n",
    "print(f\"Train: {len(df_train)}\")\n",
    "print(f\"Dev:   {len(df_dev)}\")\n",
    "print(f\"Test:  {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4ffb0989995e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.95\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.95\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.95\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bert_pos_analysis/venv/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mto_markdown\u001b[0;34m(self, buf, mode, index, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \"\"\"\n\u001b[0;32m-> 1479\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bert_pos_analysis/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_markdown\u001b[0;34m(self, buf, mode, index, **kwargs)\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tablefmt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"showindex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m         \u001b[0mtabulate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tabulate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2262\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bert_pos_analysis/venv/lib/python3.6/site-packages/pandas/compat/_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "df_train[\"word_count\"] = df_train[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "df_dev[\"word_count\"] = df_dev[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "df_test[\"word_count\"] = df_test[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "train_sent_leb_dist = df_train.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].hist(bins=100)\n",
    "dev_sent_leb_dist = df_dev.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].hist(bins=100)\n",
    "test_sent_leb_dist = df_test.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].hist(bins=100)\n",
    "\n",
    "\n",
    "plt.suptitle('Sentence Length Distribution', x=0.5, ha='center')\n",
    "fig.text(0.5, 0.02, 'Length [words]', ha='center')\n",
    "fig.text(0.02, 0.5, 'Count', va='center', rotation='vertical')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(df_train.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].describe(percentiles=[.25,.50,.75,.95]).to_markdown())\n",
    "print(df_dev.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].describe(percentiles=[.25,.50,.75,.95]).to_markdown())\n",
    "print(df_test.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].describe(percentiles=[.25,.50,.75,.95]).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, dataloader_train = text_to_dataloader(df_train, \"cuda\", 32, bert_tokenizer, 256)\n",
    "#df_test, dataloader_test = text_to_dataloader(df_test, \"cuda\", 32, bert_tokenizer, 256)\n",
    "#df_dev, dataloader_dev = text_to_dataloader(df_dev, \"cuda\", 32, bert_tokenizer, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Tokens/Word |   Count |\n",
      "|--------------:|--------:|\n",
      "|             0 |      55 |\n",
      "|             1 |   41015 |\n",
      "|             2 |    1388 |\n",
      "|             3 |     729 |\n",
      "|             4 |     161 |\n",
      "|             5 |      24 |\n",
      "|             6 |       6 |\n"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/tokens_per_word_dist_train.tex'\n",
    "\n",
    "INDEX_AXIS_NAME = \"Tokens/Word\"\n",
    "SORT_COL = \"Tokens/Word\"\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_train[\"query_mask\"].apply(lambda x: sum(x)).value_counts().rename_axis(INDEX_AXIS_NAME).to_frame(\"Count\").reset_index()\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    print(display_df.to_markdown(index=False))\n",
    "    tf.write(latex_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_offset</th>\n",
       "      <th>word_count</th>\n",
       "      <th>label_idx</th>\n",
       "      <th>text_ids</th>\n",
       "      <th>attn_mask</th>\n",
       "      <th>query_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cuthbert</td>\n",
       "      <td>in an answer to the sharers' petition in 1635 ...</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "      <td>[101, 1999, 2019, 3437, 2000, 1996, 3745, 2869...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>updates</td>\n",
       "      <td>Allowing subscribers is a simple way to broade...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>[101, 4352, 17073, 2003, 1037, 3722, 2126, 200...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>this</td>\n",
       "      <td>We know, and we have stated as much in very ma...</td>\n",
       "      <td>PRON</td>\n",
       "      <td>30</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>[101, 2057, 2113, 1010, 1998, 2057, 2031, 3090...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>produce</td>\n",
       "      <td>Thus, a decline in the number of farmers could...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>[101, 2947, 1010, 1037, 6689, 1999, 1996, 2193...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He</td>\n",
       "      <td>He seems to have planned two contrasting serie...</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>[101, 2002, 3849, 2000, 2031, 3740, 2048, 2213...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word                                               text  label  \\\n",
       "10  Cuthbert  in an answer to the sharers' petition in 1635 ...  PROPN   \n",
       "16   updates  Allowing subscribers is a simple way to broade...   NOUN   \n",
       "29      this  We know, and we have stated as much in very ma...   PRON   \n",
       "10   produce  Thus, a decline in the number of farmers could...   VERB   \n",
       "0         He  He seems to have planned two contrasting serie...   PRON   \n",
       "\n",
       "   word_offset  word_count label_idx  \\\n",
       "10          11          39        11   \n",
       "16          17          21         7   \n",
       "29          30          57        10   \n",
       "10          11          18        15   \n",
       "0            1           8        10   \n",
       "\n",
       "                                             text_ids  \\\n",
       "10  [101, 1999, 2019, 3437, 2000, 1996, 3745, 2869...   \n",
       "16  [101, 4352, 17073, 2003, 1037, 3722, 2126, 200...   \n",
       "29  [101, 2057, 2113, 1010, 1998, 2057, 2031, 3090...   \n",
       "10  [101, 2947, 1010, 1037, 6689, 1999, 1996, 2193...   \n",
       "0   [101, 2002, 3849, 2000, 2031, 3740, 2048, 2213...   \n",
       "\n",
       "                                            attn_mask  \\\n",
       "10  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "16  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "29  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "10  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "0   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           query_mask  \n",
       "10  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "16  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "29  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "0   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'query_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/bert_pos_analysis/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2888\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2889\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'query_mask'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8d886bad3758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdisplay_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINDEX_AXIS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdisplay_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSORT_COL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlatex_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_latex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bert_pos_analysis/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bert_pos_analysis/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'query_mask'"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/tokens_per_word_dist_dev.tex'\n",
    "\n",
    "INDEX_AXIS_NAME = \"Tokens/Word\"\n",
    "SORT_COL = \"Tokens/Word\"\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_dev[\"query_mask\"].apply(lambda x: sum(x)).value_counts().rename_axis(INDEX_AXIS_NAME).to_frame(\"Count\").reset_index()\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    print(display_df.to_markdown(index=False))\n",
    "    tf.write(latex_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'tex_artifacts/tokens_per_word_dist_test.tex'\n",
    "\n",
    "INDEX_AXIS_NAME = \"Tokens/Word\"\n",
    "SORT_COL = \"Tokens/Word\"\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_test[\"query_mask\"].apply(lambda x: sum(x)).value_counts().rename_axis(INDEX_AXIS_NAME).to_frame(\"Count\").reset_index()\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    print(display_df.to_markdown(index=False))\n",
    "    tf.write(latex_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Distribution of this license does not create a...\n",
       "1    Distribution of this license does not create a...\n",
       "2    Distribution of this license does not create a...\n",
       "3    Distribution of this license does not create a...\n",
       "4    Distribution of this license does not create a...\n",
       "                           ...                        \n",
       "5    No Shakespearean poems were included in the Fi...\n",
       "6    No Shakespearean poems were included in the Fi...\n",
       "7    No Shakespearean poems were included in the Fi...\n",
       "8    No Shakespearean poems were included in the Fi...\n",
       "9    No Shakespearean poems were included in the Fi...\n",
       "Name: text, Length: 43378, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43378, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[[\"text\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1356/1356 [00:36<00:00, 37.63it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1356/1356 [01:01<00:00, 22.21it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1356/1356 [01:32<00:00, 14.69it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1356/1356 [01:57<00:00, 11.50it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1356/1356 [02:30<00:00,  9.01it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1356/1356 [02:58<00:00,  7.62it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1356/1356 [03:27<00:00,  6.52it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1356/1356 [03:54<00:00,  5.78it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1356/1356 [04:24<00:00,  5.14it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1356/1356 [04:51<00:00,  4.64it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1356/1356 [05:25<00:00,  4.17it/s]\n",
      "100%|| 1356/1356 [05:48<00:00,  3.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "\n",
    "# extract bert base embedding\n",
    "for i in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "#for i in [4]:  \n",
    "    bex = BertEmbeddingExtractorVanilla(i, \"bert-base-uncased\")\n",
    "    #bex = BertEmbeddingExtractorRandom(i, \"bert-base-uncased\")\n",
    "    embedding_df = bex.extract_embedding(dataloader_train, \"sum\")\n",
    "    embedding_df.columns = [str(col) for col in embedding_df.columns]\n",
    "    embedding_df[\"text\"] = df_train[\"text\"].values\n",
    "    save_path = os.path.join(\"bert_embeddings\", f\"bert_base_embedding_layer_{i}\")\n",
    "    embedding_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43378, 771)\n",
      "(43378, 1)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_df.shape)\n",
    "print(df_train[[\"text\"]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label_idx</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distribution</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>0.264052</td>\n",
       "      <td>-0.183813</td>\n",
       "      <td>-0.974775</td>\n",
       "      <td>-0.142266</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>-0.777720</td>\n",
       "      <td>0.640850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610982</td>\n",
       "      <td>-0.777355</td>\n",
       "      <td>-0.017882</td>\n",
       "      <td>0.786865</td>\n",
       "      <td>-0.719881</td>\n",
       "      <td>-0.249511</td>\n",
       "      <td>1.240649</td>\n",
       "      <td>0.851843</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Distribution of this license does not create a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.635859</td>\n",
       "      <td>0.704649</td>\n",
       "      <td>0.369578</td>\n",
       "      <td>-0.556018</td>\n",
       "      <td>-0.658731</td>\n",
       "      <td>0.097797</td>\n",
       "      <td>0.837619</td>\n",
       "      <td>-0.916030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594430</td>\n",
       "      <td>-0.449502</td>\n",
       "      <td>0.035313</td>\n",
       "      <td>0.437873</td>\n",
       "      <td>0.342937</td>\n",
       "      <td>-0.263851</td>\n",
       "      <td>0.027576</td>\n",
       "      <td>-0.251609</td>\n",
       "      <td>ADP</td>\n",
       "      <td>Distribution of this license does not create a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.135625</td>\n",
       "      <td>0.745417</td>\n",
       "      <td>0.154091</td>\n",
       "      <td>0.732729</td>\n",
       "      <td>0.063750</td>\n",
       "      <td>0.649491</td>\n",
       "      <td>-0.011258</td>\n",
       "      <td>-0.119754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289354</td>\n",
       "      <td>-0.727232</td>\n",
       "      <td>-0.739072</td>\n",
       "      <td>0.185910</td>\n",
       "      <td>-0.406408</td>\n",
       "      <td>-0.171597</td>\n",
       "      <td>0.066590</td>\n",
       "      <td>-0.175470</td>\n",
       "      <td>DET</td>\n",
       "      <td>Distribution of this license does not create a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>license</td>\n",
       "      <td>7</td>\n",
       "      <td>1.998602</td>\n",
       "      <td>-1.701456</td>\n",
       "      <td>-0.684037</td>\n",
       "      <td>0.057087</td>\n",
       "      <td>1.182045</td>\n",
       "      <td>-2.653688</td>\n",
       "      <td>-0.985619</td>\n",
       "      <td>0.531290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866031</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>-0.342068</td>\n",
       "      <td>-1.176604</td>\n",
       "      <td>-0.640310</td>\n",
       "      <td>-0.463639</td>\n",
       "      <td>0.472609</td>\n",
       "      <td>0.380957</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Distribution of this license does not create a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>does</td>\n",
       "      <td>3</td>\n",
       "      <td>0.199792</td>\n",
       "      <td>-0.842543</td>\n",
       "      <td>-0.299119</td>\n",
       "      <td>-0.653575</td>\n",
       "      <td>-0.433593</td>\n",
       "      <td>-0.940490</td>\n",
       "      <td>-0.057850</td>\n",
       "      <td>0.142401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152853</td>\n",
       "      <td>-0.080733</td>\n",
       "      <td>-0.518532</td>\n",
       "      <td>-0.873806</td>\n",
       "      <td>0.541097</td>\n",
       "      <td>0.084881</td>\n",
       "      <td>0.212272</td>\n",
       "      <td>-0.441872</td>\n",
       "      <td>AUX</td>\n",
       "      <td>Distribution of this license does not create a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43373</th>\n",
       "      <td>in</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.113231</td>\n",
       "      <td>-0.523394</td>\n",
       "      <td>0.786649</td>\n",
       "      <td>-0.304570</td>\n",
       "      <td>-0.559754</td>\n",
       "      <td>0.341932</td>\n",
       "      <td>-0.544316</td>\n",
       "      <td>0.021857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096374</td>\n",
       "      <td>-0.653595</td>\n",
       "      <td>0.176420</td>\n",
       "      <td>0.338754</td>\n",
       "      <td>0.023564</td>\n",
       "      <td>0.270207</td>\n",
       "      <td>0.456309</td>\n",
       "      <td>0.270772</td>\n",
       "      <td>ADP</td>\n",
       "      <td>No Shakespearean poems were included in the Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43374</th>\n",
       "      <td>the</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.174330</td>\n",
       "      <td>-0.745364</td>\n",
       "      <td>0.188594</td>\n",
       "      <td>0.012049</td>\n",
       "      <td>-0.060826</td>\n",
       "      <td>-0.005593</td>\n",
       "      <td>0.034134</td>\n",
       "      <td>-0.398250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163067</td>\n",
       "      <td>-0.075282</td>\n",
       "      <td>-0.215098</td>\n",
       "      <td>0.345932</td>\n",
       "      <td>0.273631</td>\n",
       "      <td>0.707599</td>\n",
       "      <td>0.804896</td>\n",
       "      <td>-0.734730</td>\n",
       "      <td>DET</td>\n",
       "      <td>No Shakespearean poems were included in the Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43375</th>\n",
       "      <td>first</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.251023</td>\n",
       "      <td>0.242058</td>\n",
       "      <td>-0.343996</td>\n",
       "      <td>-0.846067</td>\n",
       "      <td>0.677346</td>\n",
       "      <td>-0.448930</td>\n",
       "      <td>-0.702235</td>\n",
       "      <td>0.315923</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.263426</td>\n",
       "      <td>-0.584649</td>\n",
       "      <td>-0.327501</td>\n",
       "      <td>-0.681359</td>\n",
       "      <td>0.973242</td>\n",
       "      <td>-0.085762</td>\n",
       "      <td>1.125040</td>\n",
       "      <td>-1.707617</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>No Shakespearean poems were included in the Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43376</th>\n",
       "      <td>folio</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.187790</td>\n",
       "      <td>-1.361653</td>\n",
       "      <td>0.950157</td>\n",
       "      <td>0.501484</td>\n",
       "      <td>1.043817</td>\n",
       "      <td>-0.878455</td>\n",
       "      <td>-0.486336</td>\n",
       "      <td>-0.752809</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.447888</td>\n",
       "      <td>-0.095814</td>\n",
       "      <td>-1.692210</td>\n",
       "      <td>0.593824</td>\n",
       "      <td>-2.245519</td>\n",
       "      <td>2.612092</td>\n",
       "      <td>0.640596</td>\n",
       "      <td>-0.421789</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>No Shakespearean poems were included in the Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43377</th>\n",
       "      <td>.</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.729332</td>\n",
       "      <td>-0.501128</td>\n",
       "      <td>0.436775</td>\n",
       "      <td>-0.076864</td>\n",
       "      <td>-0.694035</td>\n",
       "      <td>0.234650</td>\n",
       "      <td>0.565920</td>\n",
       "      <td>-1.130665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.715708</td>\n",
       "      <td>-1.144301</td>\n",
       "      <td>0.684151</td>\n",
       "      <td>-0.437484</td>\n",
       "      <td>-0.041664</td>\n",
       "      <td>0.459358</td>\n",
       "      <td>0.221358</td>\n",
       "      <td>0.318469</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>No Shakespearean poems were included in the Fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43378 rows  772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  label_idx         0         1         2         3  \\\n",
       "0      distribution          7 -0.046932  0.264052 -0.183813 -0.974775   \n",
       "1                of          1 -1.635859  0.704649  0.369578 -0.556018   \n",
       "2              this          5 -1.135625  0.745417  0.154091  0.732729   \n",
       "3           license          7  1.998602 -1.701456 -0.684037  0.057087   \n",
       "4              does          3  0.199792 -0.842543 -0.299119 -0.653575   \n",
       "...             ...        ...       ...       ...       ...       ...   \n",
       "43373            in          1 -1.113231 -0.523394  0.786649 -0.304570   \n",
       "43374           the          5 -1.174330 -0.745364  0.188594  0.012049   \n",
       "43375         first          0 -1.251023  0.242058 -0.343996 -0.846067   \n",
       "43376         folio          7 -0.187790 -1.361653  0.950157  0.501484   \n",
       "43377             .         12 -0.729332 -0.501128  0.436775 -0.076864   \n",
       "\n",
       "              4         5         6         7  ...       760       761  \\\n",
       "0     -0.142266 -0.000432 -0.777720  0.640850  ...  0.610982 -0.777355   \n",
       "1     -0.658731  0.097797  0.837619 -0.916030  ...  0.594430 -0.449502   \n",
       "2      0.063750  0.649491 -0.011258 -0.119754  ... -0.289354 -0.727232   \n",
       "3      1.182045 -2.653688 -0.985619  0.531290  ... -0.866031  0.192308   \n",
       "4     -0.433593 -0.940490 -0.057850  0.142401  ...  0.152853 -0.080733   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "43373 -0.559754  0.341932 -0.544316  0.021857  ... -0.096374 -0.653595   \n",
       "43374 -0.060826 -0.005593  0.034134 -0.398250  ... -0.163067 -0.075282   \n",
       "43375  0.677346 -0.448930 -0.702235  0.315923  ... -1.263426 -0.584649   \n",
       "43376  1.043817 -0.878455 -0.486336 -0.752809  ... -1.447888 -0.095814   \n",
       "43377 -0.694035  0.234650  0.565920 -1.130665  ... -0.715708 -1.144301   \n",
       "\n",
       "            762       763       764       765       766       767  label  \\\n",
       "0     -0.017882  0.786865 -0.719881 -0.249511  1.240649  0.851843   NOUN   \n",
       "1      0.035313  0.437873  0.342937 -0.263851  0.027576 -0.251609    ADP   \n",
       "2     -0.739072  0.185910 -0.406408 -0.171597  0.066590 -0.175470    DET   \n",
       "3     -0.342068 -1.176604 -0.640310 -0.463639  0.472609  0.380957   NOUN   \n",
       "4     -0.518532 -0.873806  0.541097  0.084881  0.212272 -0.441872    AUX   \n",
       "...         ...       ...       ...       ...       ...       ...    ...   \n",
       "43373  0.176420  0.338754  0.023564  0.270207  0.456309  0.270772    ADP   \n",
       "43374 -0.215098  0.345932  0.273631  0.707599  0.804896 -0.734730    DET   \n",
       "43375 -0.327501 -0.681359  0.973242 -0.085762  1.125040 -1.707617    ADJ   \n",
       "43376 -1.692210  0.593824 -2.245519  2.612092  0.640596 -0.421789   NOUN   \n",
       "43377  0.684151 -0.437484 -0.041664  0.459358  0.221358  0.318469  PUNCT   \n",
       "\n",
       "                                                    text  \n",
       "0      Distribution of this license does not create a...  \n",
       "1      Distribution of this license does not create a...  \n",
       "2      Distribution of this license does not create a...  \n",
       "3      Distribution of this license does not create a...  \n",
       "4      Distribution of this license does not create a...  \n",
       "...                                                  ...  \n",
       "43373  No Shakespearean poems were included in the Fi...  \n",
       "43374  No Shakespearean poems were included in the Fi...  \n",
       "43375  No Shakespearean poems were included in the Fi...  \n",
       "43376  No Shakespearean poems were included in the Fi...  \n",
       "43377  No Shakespearean poems were included in the Fi...  \n",
       "\n",
       "[43378 rows x 772 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"tokens_per_word\"] = df[\"query_mask\"].apply(lambda x: sum(x))\n",
    "    df[\"tokens_per_sentence\"] = df[\"attn_mask\"].apply(lambda x: sum(x))\n",
    "    #result = pd.pivot_table(df, index=[\"label\"], columns=\"tokens_per_word\" ,fill_value=0).astype(int)\n",
    "    #print(result)\n",
    "    #print(result.to_markdown())\n",
    "    print(df[\"tokens_per_sentence\"].describe(percentiles=[.25,.5,.75,.9,.95]).to_markdown())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33074</th>\n",
       "      <td>-0.214204</td>\n",
       "      <td>0.210791</td>\n",
       "      <td>-0.685075</td>\n",
       "      <td>-0.303894</td>\n",
       "      <td>0.075211</td>\n",
       "      <td>0.007617</td>\n",
       "      <td>-0.743555</td>\n",
       "      <td>-0.098195</td>\n",
       "      <td>0.024122</td>\n",
       "      <td>-0.281020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535218</td>\n",
       "      <td>-0.873311</td>\n",
       "      <td>-0.308792</td>\n",
       "      <td>0.023229</td>\n",
       "      <td>0.424931</td>\n",
       "      <td>0.496242</td>\n",
       "      <td>0.298119</td>\n",
       "      <td>0.437268</td>\n",
       "      <td>0.188881</td>\n",
       "      <td>0.081442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14344</th>\n",
       "      <td>0.691359</td>\n",
       "      <td>1.529033</td>\n",
       "      <td>0.567834</td>\n",
       "      <td>0.176478</td>\n",
       "      <td>0.145708</td>\n",
       "      <td>0.335808</td>\n",
       "      <td>-0.313668</td>\n",
       "      <td>-0.210360</td>\n",
       "      <td>-0.442215</td>\n",
       "      <td>0.601349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561651</td>\n",
       "      <td>-0.356985</td>\n",
       "      <td>-0.286182</td>\n",
       "      <td>0.561550</td>\n",
       "      <td>0.300433</td>\n",
       "      <td>-0.873633</td>\n",
       "      <td>-1.263682</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>1.279484</td>\n",
       "      <td>-0.063517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14075</th>\n",
       "      <td>-0.462501</td>\n",
       "      <td>-0.194879</td>\n",
       "      <td>0.662083</td>\n",
       "      <td>-0.156893</td>\n",
       "      <td>0.220932</td>\n",
       "      <td>-0.036545</td>\n",
       "      <td>0.410886</td>\n",
       "      <td>-0.678085</td>\n",
       "      <td>-1.071974</td>\n",
       "      <td>0.433280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709702</td>\n",
       "      <td>0.609546</td>\n",
       "      <td>1.551652</td>\n",
       "      <td>1.234532</td>\n",
       "      <td>0.142031</td>\n",
       "      <td>-0.480707</td>\n",
       "      <td>-0.794437</td>\n",
       "      <td>-0.150030</td>\n",
       "      <td>-0.132177</td>\n",
       "      <td>1.089464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25221</th>\n",
       "      <td>-0.440852</td>\n",
       "      <td>0.516707</td>\n",
       "      <td>-0.658476</td>\n",
       "      <td>-0.514138</td>\n",
       "      <td>0.731795</td>\n",
       "      <td>-0.574550</td>\n",
       "      <td>0.522442</td>\n",
       "      <td>0.089627</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>-0.378967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289961</td>\n",
       "      <td>0.239347</td>\n",
       "      <td>1.370987</td>\n",
       "      <td>1.245504</td>\n",
       "      <td>0.347981</td>\n",
       "      <td>0.667407</td>\n",
       "      <td>0.498243</td>\n",
       "      <td>-0.445687</td>\n",
       "      <td>0.631198</td>\n",
       "      <td>-0.338261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20535</th>\n",
       "      <td>-0.041180</td>\n",
       "      <td>0.481191</td>\n",
       "      <td>0.310947</td>\n",
       "      <td>-0.286946</td>\n",
       "      <td>-0.705768</td>\n",
       "      <td>-0.128789</td>\n",
       "      <td>0.393499</td>\n",
       "      <td>-0.448378</td>\n",
       "      <td>-0.730021</td>\n",
       "      <td>-0.039291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.437946</td>\n",
       "      <td>0.213805</td>\n",
       "      <td>-0.472591</td>\n",
       "      <td>-0.841031</td>\n",
       "      <td>0.294923</td>\n",
       "      <td>-0.474504</td>\n",
       "      <td>-0.183176</td>\n",
       "      <td>0.211657</td>\n",
       "      <td>0.224319</td>\n",
       "      <td>0.609070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "33074 -0.214204  0.210791 -0.685075 -0.303894  0.075211  0.007617 -0.743555   \n",
       "14344  0.691359  1.529033  0.567834  0.176478  0.145708  0.335808 -0.313668   \n",
       "14075 -0.462501 -0.194879  0.662083 -0.156893  0.220932 -0.036545  0.410886   \n",
       "25221 -0.440852  0.516707 -0.658476 -0.514138  0.731795 -0.574550  0.522442   \n",
       "20535 -0.041180  0.481191  0.310947 -0.286946 -0.705768 -0.128789  0.393499   \n",
       "\n",
       "              7         8         9  ...       758       759       760  \\\n",
       "33074 -0.098195  0.024122 -0.281020  ...  0.535218 -0.873311 -0.308792   \n",
       "14344 -0.210360 -0.442215  0.601349  ...  0.561651 -0.356985 -0.286182   \n",
       "14075 -0.678085 -1.071974  0.433280  ...  0.709702  0.609546  1.551652   \n",
       "25221  0.089627  0.000813 -0.378967  ... -0.289961  0.239347  1.370987   \n",
       "20535 -0.448378 -0.730021 -0.039291  ... -0.437946  0.213805 -0.472591   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "33074  0.023229  0.424931  0.496242  0.298119  0.437268  0.188881  0.081442  \n",
       "14344  0.561550  0.300433 -0.873633 -1.263682 -0.015551  1.279484 -0.063517  \n",
       "14075  1.234532  0.142031 -0.480707 -0.794437 -0.150030 -0.132177  1.089464  \n",
       "25221  1.245504  0.347981  0.667407  0.498243 -0.445687  0.631198 -0.338261  \n",
       "20535 -0.841031  0.294923 -0.474504 -0.183176  0.211657  0.224319  0.609070  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_bert_layer = 1\n",
    "bert_results_path = os.path.join(\"bert_embeddings\", f\"bert_base_embedding_layer_{chosen_bert_layer}\")\n",
    "\n",
    "embedding_df = pd.read_csv(bert_results_path, index_col=False)\n",
    "embedding_df.dropna(inplace=True)\n",
    "embedding_columns_list = [str(i) for i in list(range(768))]\n",
    "contextual_embedding_df = embedding_df[embedding_columns_list]\n",
    "\n",
    "contextual_embedding_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Neuron')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title(f\"Neuron Activation Value STD of {chosen_bert_layer}th Layer\")\n",
    "neuron_std = contextual_embedding_df.std()\n",
    "x = range(768)\n",
    "plt.bar(x, height=neuron_std)\n",
    "plt.ylabel(\"std\")\n",
    "plt.xlabel(\"Neuron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = contextual_embedding_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(corr_mat, fignum=f.number)\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.title(f'BERT {chosen_bert_layer} layer feature Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3160</th>\n",
       "      <td>-0.193511</td>\n",
       "      <td>0.141560</td>\n",
       "      <td>-0.073956</td>\n",
       "      <td>-0.541930</td>\n",
       "      <td>0.190930</td>\n",
       "      <td>-0.420567</td>\n",
       "      <td>-0.407052</td>\n",
       "      <td>-0.362086</td>\n",
       "      <td>0.328915</td>\n",
       "      <td>0.052562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643404</td>\n",
       "      <td>-0.357056</td>\n",
       "      <td>-0.035698</td>\n",
       "      <td>-0.613128</td>\n",
       "      <td>0.295425</td>\n",
       "      <td>-0.882804</td>\n",
       "      <td>-0.089768</td>\n",
       "      <td>-0.305801</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>0.803062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9873</th>\n",
       "      <td>0.203352</td>\n",
       "      <td>0.649730</td>\n",
       "      <td>0.217271</td>\n",
       "      <td>0.453187</td>\n",
       "      <td>0.198985</td>\n",
       "      <td>-0.267946</td>\n",
       "      <td>0.158256</td>\n",
       "      <td>-0.051970</td>\n",
       "      <td>0.531361</td>\n",
       "      <td>0.521760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158498</td>\n",
       "      <td>-0.128969</td>\n",
       "      <td>-0.432407</td>\n",
       "      <td>-0.237052</td>\n",
       "      <td>0.016998</td>\n",
       "      <td>-0.242871</td>\n",
       "      <td>-0.289553</td>\n",
       "      <td>0.153862</td>\n",
       "      <td>-0.661143</td>\n",
       "      <td>0.067906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16177</th>\n",
       "      <td>0.377852</td>\n",
       "      <td>0.616553</td>\n",
       "      <td>0.594940</td>\n",
       "      <td>-0.048526</td>\n",
       "      <td>0.550139</td>\n",
       "      <td>-0.682541</td>\n",
       "      <td>-0.133020</td>\n",
       "      <td>0.389034</td>\n",
       "      <td>0.112407</td>\n",
       "      <td>-0.103823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180712</td>\n",
       "      <td>0.495960</td>\n",
       "      <td>-0.006328</td>\n",
       "      <td>-0.746936</td>\n",
       "      <td>0.505624</td>\n",
       "      <td>-0.470204</td>\n",
       "      <td>-0.730897</td>\n",
       "      <td>-0.703259</td>\n",
       "      <td>-0.195477</td>\n",
       "      <td>0.324567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31678</th>\n",
       "      <td>0.218324</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>-0.445144</td>\n",
       "      <td>0.396352</td>\n",
       "      <td>0.082238</td>\n",
       "      <td>0.015679</td>\n",
       "      <td>0.664757</td>\n",
       "      <td>-0.168424</td>\n",
       "      <td>-0.009430</td>\n",
       "      <td>0.125564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012610</td>\n",
       "      <td>-0.103079</td>\n",
       "      <td>-0.297756</td>\n",
       "      <td>-0.839267</td>\n",
       "      <td>0.760911</td>\n",
       "      <td>-0.533729</td>\n",
       "      <td>-0.254283</td>\n",
       "      <td>-0.196726</td>\n",
       "      <td>0.416015</td>\n",
       "      <td>0.832551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17985</th>\n",
       "      <td>0.118665</td>\n",
       "      <td>0.439337</td>\n",
       "      <td>0.444369</td>\n",
       "      <td>-0.242805</td>\n",
       "      <td>-0.903796</td>\n",
       "      <td>0.407297</td>\n",
       "      <td>0.108926</td>\n",
       "      <td>0.196786</td>\n",
       "      <td>-0.217531</td>\n",
       "      <td>0.263144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386092</td>\n",
       "      <td>-0.534709</td>\n",
       "      <td>-0.043385</td>\n",
       "      <td>0.183104</td>\n",
       "      <td>0.395186</td>\n",
       "      <td>-0.487802</td>\n",
       "      <td>-0.275351</td>\n",
       "      <td>-0.219180</td>\n",
       "      <td>0.377723</td>\n",
       "      <td>0.149693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "3160  -0.193511  0.141560 -0.073956 -0.541930  0.190930 -0.420567 -0.407052   \n",
       "9873   0.203352  0.649730  0.217271  0.453187  0.198985 -0.267946  0.158256   \n",
       "16177  0.377852  0.616553  0.594940 -0.048526  0.550139 -0.682541 -0.133020   \n",
       "31678  0.218324  0.028633 -0.445144  0.396352  0.082238  0.015679  0.664757   \n",
       "17985  0.118665  0.439337  0.444369 -0.242805 -0.903796  0.407297  0.108926   \n",
       "\n",
       "              7         8         9  ...       758       759       760  \\\n",
       "3160  -0.362086  0.328915  0.052562  ...  0.643404 -0.357056 -0.035698   \n",
       "9873  -0.051970  0.531361  0.521760  ... -0.158498 -0.128969 -0.432407   \n",
       "16177  0.389034  0.112407 -0.103823  ...  0.180712  0.495960 -0.006328   \n",
       "31678 -0.168424 -0.009430  0.125564  ... -0.012610 -0.103079 -0.297756   \n",
       "17985  0.196786 -0.217531  0.263144  ... -0.386092 -0.534709 -0.043385   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "3160  -0.613128  0.295425 -0.882804 -0.089768 -0.305801  0.004325  0.803062  \n",
       "9873  -0.237052  0.016998 -0.242871 -0.289553  0.153862 -0.661143  0.067906  \n",
       "16177 -0.746936  0.505624 -0.470204 -0.730897 -0.703259 -0.195477  0.324567  \n",
       "31678 -0.839267  0.760911 -0.533729 -0.254283 -0.196726  0.416015  0.832551  \n",
       "17985  0.183104  0.395186 -0.487802 -0.275351 -0.219180  0.377723  0.149693  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_bert_layer = 12\n",
    "bert_results_path = os.path.join(\"bert_embeddings\", f\"bert_base_embedding_layer_{chosen_bert_layer}\")\n",
    "\n",
    "embedding_df = pd.read_csv(bert_results_path, index_col=False)\n",
    "embedding_df.dropna(inplace=True)\n",
    "embedding_columns_list = [str(i) for i in list(range(768))]\n",
    "contextual_embedding_df = embedding_df[embedding_columns_list]\n",
    "\n",
    "contextual_embedding_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = contextual_embedding_df.values  #(43323, 768)\n",
    "y = embedding_df[\"label_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_series = pd.Series(feature_importance, name=\"feature_importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     |   feature_importance |\n",
      "|----:|---------------------:|\n",
      "| 251 |           0.0049073  |\n",
      "| 287 |           0.00376814 |\n",
      "| 581 |           0.00310467 |\n",
      "| 374 |           0.00302757 |\n",
      "| 180 |           0.00293183 |\n",
      "| 101 |           0.00269922 |\n",
      "|   0 |           0.00268066 |\n",
      "| 616 |           0.00265702 |\n",
      "|  71 |           0.00264255 |\n",
      "| 427 |           0.00262997 |\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance_series.sort_values(ascending=False).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "title_string = f\"Neuron Feature Importance Value, {chosen_bert_layer}th Layer \\n Tree Accuracy Score: {acc:.2f}\"\n",
    "plt.title(title_string)\n",
    "x = range(768)\n",
    "plt.bar(x, height=feature_importance)\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.xlabel(\"Neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "chosen_bert_layer = 12\n",
    "umap_seed = 42\n",
    "\n",
    "\n",
    "bert_results_path = os.path.join(\"bert_embeddings\", f\"bert_base_embedding_layer_{chosen_bert_layer}\")\n",
    "\n",
    "\n",
    "query_df = embedding_df\n",
    "\n",
    "# query_df = pd.read_csv(\n",
    "#     bert_results_path\n",
    "# )\n",
    "\n",
    "query_df.dropna(inplace=True)\n",
    "\n",
    "embedding_columns_list = [str(i) for i in list(range(768))]\n",
    "contextual_embedding_array = query_df[embedding_columns_list].to_numpy()\n",
    "\n",
    "reducer = umap.UMAP(random_state=umap_seed)\n",
    "lower_dim_data = reducer.fit_transform(\n",
    "    contextual_embedding_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import cm\n",
    "import mplcursors\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "word_list = list(query_df[\"word\"].tolist())\n",
    "all_labels = query_df[\"label\"].tolist()\n",
    "labels = list(set(all_labels))\n",
    "labels.sort()\n",
    "n_colors = len(labels)\n",
    "\n",
    "#create new colormap\n",
    "cmap = cm.get_cmap('tab20', n_colors)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(4.5,4.5))\n",
    "\n",
    "sc = plt.scatter(\n",
    "    lower_dim_data[:,0], \n",
    "    lower_dim_data[:,1], \n",
    "    c=query_df[\"label_idx\"].tolist(),\n",
    "    cmap=cmap,\n",
    "    s=1\n",
    ")\n",
    "\n",
    "# cursor that show the word when hovering over it\n",
    "crs = mplcursors.cursor(ax,hover=True)\n",
    "crs.connect(\n",
    "    \"add\", \n",
    "    lambda sel: sel.annotation.set_text(\n",
    "        f\"{word_list[sel.target.index]}\\n{all_labels[sel.target.index]}\"\n",
    "    ))\n",
    "    \n",
    "#title\n",
    "plt.title(f\"{chosen_bert_layer}th layer intermediate representation\")\n",
    "\n",
    "# colorbar\n",
    "c_ticks = np.arange(n_colors) * (n_colors / (n_colors + 1)) + (2 / n_colors)\n",
    "cbar = plt.colorbar(sc, ticks=c_ticks)\n",
    "ticklabs = cbar.ax.get_yticklabels()\n",
    "cbar.ax.set_yticklabels(labels, ha=\"right\")\n",
    "cbar.ax.yaxis.set_tick_params(pad=40)\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "name = f\"BERT Contextual Embedding Visualization of the {chosen_bert_layer}th Layer\"\n",
    "plt.savefig(name + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "1170px",
    "left": "51px",
    "right": "20px",
    "top": "119px",
    "width": "559px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
