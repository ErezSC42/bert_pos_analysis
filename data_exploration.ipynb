{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import check_accuracy_classification\n",
    "import transformers\n",
    "from torch.optim import Adam\n",
    "from models import BertProbeClassifer\n",
    "from utils import text_to_dataloader, tokenize_word\n",
    "from bert_embedding import BertEmbeddingExtractorRandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"data\",\"en_partut-ud-train.conllu\")\n",
    "dev_path = os.path.join(\"data\",\"en_partut-ud-dev.conllu\")\n",
    "test_path = os.path.join(\"data\",\"en_partut-ud-test.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER_CONST = \"# sent_id = \"\n",
    "TEXT_CONST = \"# text = \"\n",
    "STOP_CONST = \"\\n\"\n",
    "WORD_OFFSET = 1\n",
    "LABEL_OFFSET = 3\n",
    "NUM_OFFSET = 0\n",
    "\n",
    "\n",
    "def txt_to_dataframe(data_path):\n",
    "    '''\n",
    "    read UD text file and convert to df format\n",
    "    '''\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        df = pd.DataFrame(\n",
    "            columns={\n",
    "                \"text\",\n",
    "                \"word\",\n",
    "                \"label\"\n",
    "            }\n",
    "        )\n",
    "        for line in fp.readlines():\n",
    "            if TEXT_CONST in line:\n",
    "                words_list = []\n",
    "                labels_list = []\n",
    "                num_list = []\n",
    "                text = line.split(TEXT_CONST)[1]\n",
    "                # this is a new text, need to parse all the words in it\n",
    "            elif line is not STOP_CONST and HEADER_CONST not in line:\n",
    "                temp_list = line.split(\"\\t\")\n",
    "                num_list.append(temp_list[NUM_OFFSET])\n",
    "                words_list.append(temp_list[WORD_OFFSET])\n",
    "                labels_list.append(temp_list[LABEL_OFFSET])\n",
    "            if line == STOP_CONST:\n",
    "                # this is the end of the text, adding to df\n",
    "                cur_df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"text\": len(words_list) * [text],\n",
    "                        \"word\": words_list,\n",
    "                        \"word_offset\": num_list,\n",
    "                        \"label\": labels_list\n",
    "                    }\n",
    "                )\n",
    "                df = pd.concat([df,cur_df])\n",
    "        return df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = txt_to_dataframe(train_path)\n",
    "df_dev = txt_to_dataframe(dev_path)\n",
    "df_test = txt_to_dataframe(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = [\n",
    "    \"ADJ\",\n",
    "    \"ADP\",\n",
    "    \"ADV\",\n",
    "    \"AUX\",\n",
    "    \"CCONJ\",\n",
    "    \"DET\",\n",
    "    \"INTJ\",\n",
    "    \"NOUN\",\n",
    "    \"NUM\",\n",
    "    \"PART\",\n",
    "    \"PRON\",\n",
    "    \"PROPN\",\n",
    "    \"PUNCT\",\n",
    "    \"SCONJ\",\n",
    "    \"SYM\",\n",
    "    \"VERB\",\n",
    "    \"X\",\n",
    "    \"_\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes, Mrs Schroedter, I shall be pleased to loo...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>etc.</td>\n",
       "      <td>For this reason, one of the most important and...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>right</td>\n",
       "      <td>We know that, right?\\n</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>yes</td>\n",
       "      <td>In developing countries that have not yet reac...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes, being better connected with each other, t...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>etc.</td>\n",
       "      <td>in an answer to the sharers' petition in 1635 ...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word                                               text label word_offset\n",
       "0     Yes  Yes, Mrs Schroedter, I shall be pleased to loo...  INTJ           1\n",
       "62   etc.  For this reason, one of the most important and...  INTJ          63\n",
       "4   right                             We know that, right?\\n  INTJ           5\n",
       "17    yes  In developing countries that have not yet reac...  INTJ          18\n",
       "0     Yes  Yes, being better connected with each other, t...  INTJ           1\n",
       "48   etc.  in an answer to the sharers' petition in 1635 ...  INTJ          49"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train[\"label\"] == \"INTJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Type   |   Count |\n",
      "|:-------|--------:|\n",
      "| NOUN   |    9249 |\n",
      "| ADP    |    5220 |\n",
      "| PUNCT  |    5105 |\n",
      "| DET    |    4616 |\n",
      "| VERB   |    4126 |\n",
      "| ADJ    |    3410 |\n",
      "| AUX    |    2076 |\n",
      "| PROPN  |    2033 |\n",
      "| PRON   |    1734 |\n",
      "| ADV    |    1707 |\n",
      "| CCONJ  |    1472 |\n",
      "| PART   |    1168 |\n",
      "| NUM    |     787 |\n",
      "| SCONJ  |     627 |\n",
      "| X      |     140 |\n",
      "| SYM    |      42 |\n",
      "| _      |      27 |\n",
      "| INTJ   |       6 |\n"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/label_dist_train.tex'\n",
    "SORT_COL = \"Count\"\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_train[\"label\"].value_counts().rename_axis(\"Type\").to_frame(\"Count\").reset_index()\n",
    "    #display_df.index = TYPES\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True, ascending=False)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    tf.write(latex_data)\n",
    "    print(display_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Type   |   Count |\n",
      "|:-------|--------:|\n",
      "| NOUN   |     568 |\n",
      "| PUNCT  |     353 |\n",
      "| ADP    |     297 |\n",
      "| VERB   |     276 |\n",
      "| DET    |     266 |\n",
      "| ADJ    |     210 |\n",
      "| PRON   |     153 |\n",
      "| AUX    |     124 |\n",
      "| ADV    |     108 |\n",
      "| PROPN  |     107 |\n",
      "| CCONJ  |      88 |\n",
      "| NUM    |      60 |\n",
      "| PART   |      56 |\n",
      "| SCONJ  |      41 |\n",
      "| X      |      13 |\n",
      "| SYM    |       2 |\n",
      "| _      |       1 |\n"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/label_dist_dev.tex'\n",
    "\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_dev[\"label\"].value_counts().rename_axis(\"Type\").to_frame(\"Count\").reset_index()\n",
    "    #display_df.index = TYPES\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True, ascending=False)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    tf.write(latex_data)\n",
    "    print(display_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Type   |   Count |\n",
      "|:-------|--------:|\n",
      "| NOUN   |     753 |\n",
      "| ADP    |     488 |\n",
      "| DET    |     439 |\n",
      "| PUNCT  |     339 |\n",
      "| VERB   |     326 |\n",
      "| AUX    |     234 |\n",
      "| ADJ    |     224 |\n",
      "| ADV    |     131 |\n",
      "| PRON   |     106 |\n",
      "| CCONJ  |      96 |\n",
      "| PROPN  |      90 |\n",
      "| PART   |      66 |\n",
      "| NUM    |      61 |\n",
      "| SCONJ  |      51 |\n",
      "| _      |       4 |\n",
      "| X      |       2 |\n",
      "| INTJ   |       2 |\n"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/label_dist_test.tex'\n",
    "\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_test[\"label\"].value_counts().rename_axis(\"Type\").to_frame(\"Count\").reset_index()\n",
    "    #display_df.index = TYPES\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True, ascending=False)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    tf.write(latex_data)\n",
    "    print(display_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences:\n",
      "Train: 1780\n",
      "Dev:   156\n",
      "Test:  153\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Sentences:\")\n",
    "print(f\"Train: {len(df_train.drop_duplicates(subset='text', keep='first'))}\")\n",
    "print(f\"Dev:   {len(df_dev.drop_duplicates(subset='text', keep='first'))}\")\n",
    "print(f\"Test:  {len(df_test.drop_duplicates(subset='text', keep='first'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words:\n",
      "Train: 43545\n",
      "Dev:   2723\n",
      "Test:  3412\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Words:\")\n",
    "print(f\"Train: {len(df_train)}\")\n",
    "print(f\"Dev:   {len(df_dev)}\")\n",
    "print(f\"Test:  {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       |   word_count |\n",
      "|:------|-------------:|\n",
      "| count |    1780      |\n",
      "| mean  |      21.2225 |\n",
      "| std   |      12.6585 |\n",
      "| min   |       1      |\n",
      "| 25%   |      13      |\n",
      "| 50%   |      19      |\n",
      "| 75%   |      27      |\n",
      "| 95%   |      43      |\n",
      "| max   |     173      |\n",
      "|       |   word_count |\n",
      "|:------|-------------:|\n",
      "| count |    156       |\n",
      "| mean  |     14.8526  |\n",
      "| std   |      8.60068 |\n",
      "| min   |      2       |\n",
      "| 25%   |      8       |\n",
      "| 50%   |     13       |\n",
      "| 75%   |     20       |\n",
      "| 95%   |     31.25    |\n",
      "| max   |     39       |\n",
      "|       |   word_count |\n",
      "|:------|-------------:|\n",
      "| count |     153      |\n",
      "| mean  |      19.8824 |\n",
      "| std   |       9.7427 |\n",
      "| min   |       2      |\n",
      "| 25%   |      14      |\n",
      "| 50%   |      18      |\n",
      "| 75%   |      25      |\n",
      "| 95%   |      35      |\n",
      "| max   |      64      |\n"
     ]
    }
   ],
   "source": [
    "df_train[\"word_count\"] = df_train[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "df_dev[\"word_count\"] = df_dev[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "df_test[\"word_count\"] = df_test[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "train_sent_leb_dist = df_train.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].hist(bins=100)\n",
    "dev_sent_leb_dist = df_dev.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].hist(bins=100)\n",
    "test_sent_leb_dist = df_test.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].hist(bins=100)\n",
    "\n",
    "\n",
    "plt.suptitle('Sentence Length Distribution', x=0.5, ha='center')\n",
    "fig.text(0.5, 0.02, 'Length [words]', ha='center')\n",
    "fig.text(0.02, 0.5, 'Count', va='center', rotation='vertical')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(df_train.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].describe(percentiles=[.25,.50,.75,.95]).to_markdown())\n",
    "print(df_dev.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].describe(percentiles=[.25,.50,.75,.95]).to_markdown())\n",
    "print(df_test.drop_duplicates(subset=\"text\", keep=\"first\")[\"word_count\"].describe(percentiles=[.25,.50,.75,.95]).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, dataloader_train = text_to_dataloader(df_train, \"cuda\", 32, bert_tokenizer, 256)\n",
    "#df_test, dataloader_test = text_to_dataloader(df_test, \"cuda\", 32, bert_tokenizer, 256)\n",
    "#df_dev, dataloader_dev = text_to_dataloader(df_dev, \"cuda\", 32, bert_tokenizer, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Tokens/Word |   Count |\n",
      "|--------------:|--------:|\n",
      "|             0 |      55 |\n",
      "|             1 |   41015 |\n",
      "|             2 |    1388 |\n",
      "|             3 |     729 |\n",
      "|             4 |     161 |\n",
      "|             5 |      24 |\n",
      "|             6 |       6 |\n"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/tokens_per_word_dist_train.tex'\n",
    "\n",
    "INDEX_AXIS_NAME = \"Tokens/Word\"\n",
    "SORT_COL = \"Tokens/Word\"\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_train[\"query_mask\"].apply(lambda x: sum(x)).value_counts().rename_axis(INDEX_AXIS_NAME).to_frame(\"Count\").reset_index()\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    print(display_df.to_markdown(index=False))\n",
    "    tf.write(latex_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_offset</th>\n",
       "      <th>word_count</th>\n",
       "      <th>label_idx</th>\n",
       "      <th>text_ids</th>\n",
       "      <th>attn_mask</th>\n",
       "      <th>query_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cuthbert</td>\n",
       "      <td>in an answer to the sharers' petition in 1635 ...</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "      <td>[101, 1999, 2019, 3437, 2000, 1996, 3745, 2869...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>updates</td>\n",
       "      <td>Allowing subscribers is a simple way to broade...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>[101, 4352, 17073, 2003, 1037, 3722, 2126, 200...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>this</td>\n",
       "      <td>We know, and we have stated as much in very ma...</td>\n",
       "      <td>PRON</td>\n",
       "      <td>30</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>[101, 2057, 2113, 1010, 1998, 2057, 2031, 3090...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>produce</td>\n",
       "      <td>Thus, a decline in the number of farmers could...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>[101, 2947, 1010, 1037, 6689, 1999, 1996, 2193...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He</td>\n",
       "      <td>He seems to have planned two contrasting serie...</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>[101, 2002, 3849, 2000, 2031, 3740, 2048, 2213...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word                                               text  label  \\\n",
       "10  Cuthbert  in an answer to the sharers' petition in 1635 ...  PROPN   \n",
       "16   updates  Allowing subscribers is a simple way to broade...   NOUN   \n",
       "29      this  We know, and we have stated as much in very ma...   PRON   \n",
       "10   produce  Thus, a decline in the number of farmers could...   VERB   \n",
       "0         He  He seems to have planned two contrasting serie...   PRON   \n",
       "\n",
       "   word_offset  word_count label_idx  \\\n",
       "10          11          39        11   \n",
       "16          17          21         7   \n",
       "29          30          57        10   \n",
       "10          11          18        15   \n",
       "0            1           8        10   \n",
       "\n",
       "                                             text_ids  \\\n",
       "10  [101, 1999, 2019, 3437, 2000, 1996, 3745, 2869...   \n",
       "16  [101, 4352, 17073, 2003, 1037, 3722, 2126, 200...   \n",
       "29  [101, 2057, 2113, 1010, 1998, 2057, 2031, 3090...   \n",
       "10  [101, 2947, 1010, 1037, 6689, 1999, 1996, 2193...   \n",
       "0   [101, 2002, 3849, 2000, 2031, 3740, 2048, 2213...   \n",
       "\n",
       "                                            attn_mask  \\\n",
       "10  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "16  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "29  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "10  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "0   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           query_mask  \n",
       "10  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "16  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "29  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "0   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'query_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/bert_pos_analysis/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2888\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2889\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'query_mask'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8d886bad3758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdisplay_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINDEX_AXIS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdisplay_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSORT_COL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlatex_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_latex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bert_pos_analysis/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bert_pos_analysis/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'query_mask'"
     ]
    }
   ],
   "source": [
    "file_name = 'tex_artifacts/tokens_per_word_dist_dev.tex'\n",
    "\n",
    "INDEX_AXIS_NAME = \"Tokens/Word\"\n",
    "SORT_COL = \"Tokens/Word\"\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_dev[\"query_mask\"].apply(lambda x: sum(x)).value_counts().rename_axis(INDEX_AXIS_NAME).to_frame(\"Count\").reset_index()\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    print(display_df.to_markdown(index=False))\n",
    "    tf.write(latex_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'tex_artifacts/tokens_per_word_dist_test.tex'\n",
    "\n",
    "INDEX_AXIS_NAME = \"Tokens/Word\"\n",
    "SORT_COL = \"Tokens/Word\"\n",
    "\n",
    "with open(file_name,'w') as tf:\n",
    "    display_df = df_test[\"query_mask\"].apply(lambda x: sum(x)).value_counts().rename_axis(INDEX_AXIS_NAME).to_frame(\"Count\").reset_index()\n",
    "    display_df.sort_values(by=SORT_COL, inplace=True)\n",
    "    latex_data = display_df.to_latex(index=False)\n",
    "    print(display_df.to_markdown(index=False))\n",
    "    tf.write(latex_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "\n",
    "# extract bert base embedding\n",
    "#for i in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "for i in [12]:  \n",
    "    #bex = BertEmbeddingExtractor(i, \"bert-base-uncased\")\n",
    "    bex = BertEmbeddingExtractorRandom(i, \"bert-base-uncased\")\n",
    "    embedding_df = bex.extract_embedding(dataloader_train, \"sum\")\n",
    "    embedding_df.columns = [str(col) for col in embedding_df.columns]\n",
    "#     save_path = os.path.join(\"bert_embeddings\", f\"bert_base_embedding_layer_{i}\")\n",
    "#     embedding_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"tokens_per_word\"] = df[\"query_mask\"].apply(lambda x: sum(x))\n",
    "    df[\"tokens_per_sentence\"] = df[\"attn_mask\"].apply(lambda x: sum(x))\n",
    "    #result = pd.pivot_table(df, index=[\"label\"], columns=\"tokens_per_word\" ,fill_value=0).astype(int)\n",
    "    #print(result)\n",
    "    #print(result.to_markdown())\n",
    "    print(df[\"tokens_per_sentence\"].describe(percentiles=[.25,.5,.75,.9,.95]).to_markdown())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_bert_layer = 12\n",
    "bert_results_path = os.path.join(\"bert_embeddings\", f\"bert_base_embedding_layer_{chosen_bert_layer}\")\n",
    "\n",
    "embedding_df = pd.read_csv(bert_results_path, index_col=False)\n",
    "embedding_df.dropna(inplace=True)\n",
    "embedding_columns_list = [str(i) for i in list(range(768))]\n",
    "contextual_embedding_df = embedding_df[embedding_columns_list]\n",
    "\n",
    "contextual_embedding_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title(f\"Neuron Activation Value STD of {chosen_bert_layer}th Layer\")\n",
    "neuron_std = contextual_embedding_df.std()\n",
    "x = range(768)\n",
    "plt.bar(x, height=neuron_std)\n",
    "plt.ylabel(\"std\")\n",
    "plt.xlabel(\"Neuron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = contextual_embedding_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(corr_mat, fignum=f.number)\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.title(f'BERT {chosen_bert_layer} layer feature Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_bert_layer = 4\n",
    "bert_results_path = os.path.join(\"bert_embeddings\", f\"bert_base_embedding_layer_{chosen_bert_layer}\")\n",
    "\n",
    "embedding_df = pd.read_csv(bert_results_path, index_col=False)\n",
    "embedding_df.dropna(inplace=True)\n",
    "embedding_columns_list = [str(i) for i in list(range(768))]\n",
    "contextual_embedding_df = embedding_df[embedding_columns_list]\n",
    "\n",
    "contextual_embedding_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = contextual_embedding_df.values  #(43323, 768)\n",
    "y = embedding_df[\"label_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "title_string = f\"Neuron Feature Importance Value, {chosen_bert_layer}th Layer \\n Tree Accuracy Score: {acc:.2f}\"\n",
    "plt.title(title_string)\n",
    "x = range(768)\n",
    "plt.bar(x, height=feature_importance)\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.xlabel(\"Neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "chosen_bert_layer = 12\n",
    "umap_seed = 42\n",
    "\n",
    "\n",
    "bert_results_path = os.path.join(\"bert_embeddings\", f\"bert_base_embedding_layer_{chosen_bert_layer}\")\n",
    "\n",
    "\n",
    "query_df = embedding_df\n",
    "\n",
    "# query_df = pd.read_csv(\n",
    "#     bert_results_path\n",
    "# )\n",
    "\n",
    "query_df.dropna(inplace=True)\n",
    "\n",
    "embedding_columns_list = [str(i) for i in list(range(768))]\n",
    "contextual_embedding_array = query_df[embedding_columns_list].to_numpy()\n",
    "\n",
    "reducer = umap.UMAP(random_state=umap_seed)\n",
    "lower_dim_data = reducer.fit_transform(\n",
    "    contextual_embedding_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import cm\n",
    "import mplcursors\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "word_list = list(query_df[\"word\"].tolist())\n",
    "all_labels = query_df[\"label\"].tolist()\n",
    "labels = list(set(all_labels))\n",
    "labels.sort()\n",
    "n_colors = len(labels)\n",
    "\n",
    "#create new colormap\n",
    "cmap = cm.get_cmap('tab20', n_colors)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(4.5,4.5))\n",
    "\n",
    "sc = plt.scatter(\n",
    "    lower_dim_data[:,0], \n",
    "    lower_dim_data[:,1], \n",
    "    c=query_df[\"label_idx\"].tolist(),\n",
    "    cmap=cmap,\n",
    "    s=1\n",
    ")\n",
    "\n",
    "# cursor that show the word when hovering over it\n",
    "crs = mplcursors.cursor(ax,hover=True)\n",
    "crs.connect(\n",
    "    \"add\", \n",
    "    lambda sel: sel.annotation.set_text(\n",
    "        f\"{word_list[sel.target.index]}\\n{all_labels[sel.target.index]}\"\n",
    "    ))\n",
    "    \n",
    "#title\n",
    "plt.title(f\"{chosen_bert_layer}th layer intermediate representation\")\n",
    "\n",
    "# colorbar\n",
    "c_ticks = np.arange(n_colors) * (n_colors / (n_colors + 1)) + (2 / n_colors)\n",
    "cbar = plt.colorbar(sc, ticks=c_ticks)\n",
    "ticklabs = cbar.ax.get_yticklabels()\n",
    "cbar.ax.set_yticklabels(labels, ha=\"right\")\n",
    "cbar.ax.yaxis.set_tick_params(pad=40)\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "name = f\"BERT Contextual Embedding Visualization of the {chosen_bert_layer}th Layer\"\n",
    "plt.savefig(name + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "1170px",
    "left": "51px",
    "right": "20px",
    "top": "119px",
    "width": "559px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
